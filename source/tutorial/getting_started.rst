.. _tutorial_getting_started:

***************
Getting Started
***************

.. todo::

    Add typical errors on each step and how to handle them

To get started with SensorBee, this chapter introduces word counting as the first
tutorial. It covers the following topics:

* How to install and set up SensorBee
* How to build a custom ``sensorbee`` command
* How to use the ``sensorbee`` command
* How to query the SensorBee server with ``sensorbee shell`` and BQL

Prerequisites
=============

SensorBee requires Go 1.4 or later to be installed and its development
environment (``$GOPATH`` etc.) to be set up correctly. Also, Git needs
to be installed.

This tutorial assumes that readers know about basic Linux commands and basics
of SQL.

SensorBee itself doesn't have to be installed at this point.

.. todo::

    Explain more about Go in detail because not all users of SensorBee are
    familiar with Go although developers needs to know it.


Word Count Example
==================

As the first tutorial, this section shows a word count example. All programs
and configuration files required for this tutorial are provided in the
``wordcount`` package of the Github repository
`<https://github.com/sensorbee/tutorial>`_.

Installing Word Count Example Package
-------------------------------------

The first thing that needs to be done is to ``go get`` the word count example
package in the repository::

    $ go get github.com/sensorbee/tutorial/wordcount

This command clones the repository to
``$GOPATH/src/github.com/sensorbee/tutorial/wordcount`` and also downloads all
dependencies. In the ``config`` subdirectory of that path, there are
configuration files for building and running SensorBee. After ``go get``
successfully downloaded the package, copy those configuration files
to another temporary directory (**replace /path/to/ with an appropriate path**)::

    $ mkdir -p /path/to/wordcount
    $ cp $GOPATH/src/github.com/sensorbee/tutorial/wordcount/config/* \
        /path/to/wordcount/
    $ ls /path/to/wordcount
    build.yaml
    sensorbee.yaml
    wordcount.bql

Everything necessary to try this tutorial is ready now except SensorBee. The
next step is to build a custom ``sensorbee`` command that includes the plugins
needed for this tutorial.

Building a ``sensorbee`` Executable
-----------------------------------

To build a ``sensorbee`` executable, the ``build_sensorbee`` program needs to
be installed. To do so, issue the following command::

    $ go get gopkg.in/sensorbee/sensorbee.v0/...

The ``build_sensorbee`` program is used to build a custom ``sensorbee``
executable with plugins provided by developers.

Then, move to the directory that has configuration files previously copied from
the tutorial package and execute ``build_sensorbee``::

    $ cd /path/to/wordcount
    /path/to/wordcount$ build_sensorbee
    /path/to/wordcount$ ls
    build.yaml
    sensorbee
    sensorbee.yaml
    sensorbee_main.go
    wordcount.bql

There are two new files in the directory: ``sensorbee`` and
``sensorbee_main.go``. Both of them are automatically generated by the
``build_sensorbee`` command. ``sensorbee`` is the command to run the SensorBee
server or shell. Under the hood, this command is built from ``sensorbee_main.go``
using ``go build``.

``build_sensorbee`` builds a ``sensorbee`` command according to the configuration
in ``build.yaml``::

    /path/to/wordcount$ cat build.yaml
    plugins:
      - github.com/sensorbee/tutorial/wordcount/plugin

Inserting a new go path to the ``plugin`` section adds a new plugin to the
``sensorbee`` command, but this tutorial only uses the wordcount plugin above.
Other tutorials will cover this configuration file in more depth.

Run the Server
--------------

After building the ``sensorbee`` command having plugins for this tutorial,
run it as a server::

    /path/to/wordcount$ ./sensorbee run
    INFO[0000] Setting up the server context                 config={"logging":
    {"log_dropped_tuples":false,"min_log_level":"info","summarize_dropped_tuples":
    false,"target":"stderr"},"network":{"listen_on":":15601"},"storage":{"uds":
    {"params":{},"type":"in_memory"}},"topologies":{}}
    INFO[0000] Starting the server on :15601

``sensorbee run`` runs the SensorBee server. It writes some log messages to
stdout but they can be ignored at the moment. It provides a HTTP JSON API and
listens on ``:15601`` by default. However, the API isn't directly used in this
tutorial. Instead of controlling the server via the API, this tutorial shows
how to use the ``sensorbee shell`` command and the **BQL** language, which is similar
to SQL but has some extensions for streaming data.

To test if the server has successfully started, run the following command in
another terminal::

    $ curl http://localhost:15601/api/v1/runtime_status
    {"gomaxprocs":1,"goroot":"/home/pfn/go","goversion":"go1.4.2",
    "hostname":"sensorbee-tutorial","num_cgo_call":0,"num_cpu":4,
    "num_goroutine":13,"pid":33267,"user":"pfn",
    "working_directory":"/path/to/wordcount/"}

The server is correctly working if a response like this returned.

.. todo::

    Provide a /api/v1/status or something like /api/v1/ping to make the document
    simpler.

Setting Up a Topology
---------------------

Once the server has started, open another window or use screen/tmux to have
another terminal to interact with the server. The server does nothing just after
it started up. There are a few steps required to enjoy interacting
with stream data.

Firstly, to allow the server to process some data, it needs to have
a **topology**. A topology is a similar concept to a "database" in RDBMSs. It has
processing components such as data sources, continuous views, and so on.
Use the ``sensorbee topology create`` command to create a new topology
``wordcount`` for the tutorial::

    /path/to/wordcount$ ./sensorbee topology create wordcount
    /path/to/wordcount$ echo $?
    0

``$?`` (the return code of the ``./sensorbee`` command) will be ``0`` if
the command was successful. Otherwise, it will be non-zero.
Be careful to write ``./sensorbee`` (and not omit the ``./``) in order to use
the executable from your current directory, which has the correct plugins baked in.

.. note::

    Almost everything in SensorBee is volatile at the moment and is reset
    every time the server restarts. A topology is dropped when the server shuts
    down, too. Therefore, ``sensorbee topology create wordcount`` needs to be
    run on each startup of the server until we specify a config file to
    ``sensorbee run`` later.

In the next step, start ``sensorbee shell``::

    /path/to/wordcount$ ./sensorbee shell -t wordcount
    wordcount>

``-t wordcount`` means that the shell connects to the ``wordcount`` topology
just created. Now it's time to try some BQL statements. To start, try the ``EVAL``
statement, which evaluates arbitrary expressions supported by BQL::

    wordcount> EVAL 1 + 1;
    2
    wordcount> EVAL power(2.0, 2.5);
    5.65685424949238
    wordcount> EVAL "Hello" || ", world!";
    "Hello, world!"

BQL also supports one line comments::

    wordcount> -- This is a comment
    wordcount>

Finally, create a source which generates stream data or reads input data from other
stream data sources::

    wordcount> CREATE SOURCE sentences TYPE wc_sentences;
    wordcount>

This ``CREATE SOURCE`` statement creates a source named ``sentences``. Its type
is ``wc_sentences`` and it is provided by a plugin in the ``wordcount`` package.
This source emits, on a regular basis, a random sentence having several words
with the name of a person who wrote a sentence. To receive data (i.e. tuples)
emitted from the source, use the ``SELECT`` statement::

    wordcount> SELECT RSTREAM * FROM sentences [RANGE 1 TUPLES];
    {"name":"isabella","text":"dolor consequat ut in ad in"}
    {"name":"sophia","text":"excepteur deserunt officia cillum lorem excepteur"}
    {"name":"sophia","text":"exercitation ut sed aute ullamco aliquip"}
    {"name":"jacob","text":"duis occaecat culpa dolor veniam elit"}
    {"name":"isabella","text":"dolore laborum in consectetur amet ut nostrud ullamco"}
    ...

Type ``C-c`` (also known as ``Ctrl+C`` to some people) to stop the statement.
Details of the statement are not described for
now, but this is basically same as the ``SELECT`` statement in SQL except two
things: ``RSTREAM`` and ``RANGE``. Those concepts will briefly be explained in
the next section.

Querying: Basics
----------------

This subsection introduces basics of querying with BQL, i.e., the ``SELECT`` statement.
Since it is very similar to SQL's ``SELECT`` and some basic familiarity with
SQL is assumed, two concepts that don't exist in SQL are described first.
Then, some features that are also present in SQL will be covered.

Stream-Related Operators
^^^^^^^^^^^^^^^^^^^^^^^^

BQL's ``SELECT`` statement has two components related to stream data processing:
**stream-to-relation operators** and **relation-to-stream operators**.

.. note::

    Skip the description of stream-to-relations and relation-to-stream operators
    if these aren't clear enough at the moment.

A stream-to-relation operator is a operator that literally converts a stream of
tuples to relations (i.e., records in a table of the database). What it
actually does is to define a window having a finite set of tuples on a stream.
The operator is written as ``[RANGE n TUPLES]`` or ``[RANGE n SECONDS]``.
``[RANGE n TUPLES]`` creates a window containing the last :math:`n` tuples
in the stream. ``[RANGE n SECONDS]``, on the other hand, creates a window holding
tuples observed in past :math:`n` seconds (more precisely, the duration between the
oldest and newest tuple is at most :math:`n` seconds).

::

    SELECT RSTREAM * FROM sentences [RANGE 1 TUPLES];

The previous example uses a stream-to-relation operator
``[RANGE 1 TUPLES]``, i.e., each window only has a single tuple in it. This
window can be thought of as the input relation for a SQL-like ``SELECT`` statement.

Another concept that doesn't exist in SQL is a relation-to-stream operator. It
converts a relation, which is a result of the ``SELECT`` statement, to a stream
of tuples. There are three types of operators:

* ``RSTREAM``
* ``ISTREAM``
* ``DSTREAM``

``RSTREAM`` emits all tuples in the relation resulting every time a new tuple
arrives and the result is updated. ``ISTREAM`` only emits tuples that are in the
current window and weren't in the previous window, that is, it emits tuples
having newly been inserted into the current relation. ``DSTREAM`` only emits
tuples in the previous relation, that is, it emits tuples deleted in the current
relation.

In the previous example, ``RSTREAM`` is used as a relation-to-stream operator.
Since the resulting relation is same as the input relation (i.e. window), it
only has one tuple in it.

.. note::

    The difference between using ``RSTREAM`` and ``ISTREAM`` should be
    described a little here. Assume that a source ``s`` emits following 4
    tuples with timestamps :math:`t_1` to :math:`t_4`::

        t1: {"a": 1}
        t2: {"a": 2}
        t3: {"a": 2}
        t4: {"a": 3}

    When selecting these tuples by

    ::

        SELECT RSTREAM * FROM s [RANGE 1 TUPLES];

    the resulting output for each timestamp would be::

        t1: {"a": 1}
        t2: {"a": 2}
        t3: {"a": 2}
        t4: {"a": 3}

    These tuples are identical to what the source ``s`` has emitted. On the
    other hand, when ``ISTREAM`` is used instead of ``RSTREAM`` in the
    previous ``SELECT`` statement, the statement emits only three tuples::

        t1: {"a": 1}
        t2: {"a": 2}
        t4: {"a": 3}

    The reason why it happens is that the resulting relation wasn't updated at
    :math:`t_3` since both relations at :math:`t_2` and :math:`t_3` have
    the same tuple ``{"a": 2}`` as a result.

    In other words, when using ``ISTREAM`` with ``[RANGE 1 TUPLES]``, a
    resulting tuple is emitted only when it's different from the previous
    resulting tuple. In contrast, ``RSTREAM`` emits the resulting tuple every
    time regardless of its value.

    Therefore, when the stream-to-relation operator is ``[RANGE 1 TUPLES]``,
    basically prefer ``RSTREAM`` to ``ISTREAM`` unless there's a strong reason
    to use ``ISTREAM``. It leads to less confusing results.

To learn more about these operators, see :ref:`bql_queries` after finishing this
tutorial.

Selection
^^^^^^^^^

The ``SELECT`` statement can partially pick up some fields of input tuples::

    wordcount> SELECT RSTREAM name FROM sentences [RANGE 1 TUPLES];
    {"name":"isabella"}
    {"name":"isabella"}
    {"name":"jacob"}
    {"name":"isabella"}
    {"name":"jacob"}
    ...

In this example, only the ``name`` field is picked up from input tuples that
have "name" and "text" fields.

BQL is schema-less at the moment and the format of output tuples emitted by a
source must be documented by that source's author. The ``SELECT`` statement is only able
to report an error at runtime when processing a tuple, not at the time when it is
sent to the server. This is a drawback of being schema-less.

Filtering
^^^^^^^^^

The ``SELECT`` statement supports filtering with the ``WHERE`` clause as SQL
does::

    wordcount> SELECT RSTREAM * FROM sentences [RANGE 1 TUPLES] WHERE name = "sophia";
    {"name":"sophia","text":"anim eu occaecat do est enim do ea mollit"}
    {"name":"sophia","text":"cupidatat et mollit consectetur minim et ut deserunt"}
    {"name":"sophia","text":"elit est laborum proident deserunt eu sed consectetur"}
    {"name":"sophia","text":"mollit ullamco ut sunt sit in"}
    {"name":"sophia","text":"enim proident cillum tempor esse occaecat exercitation"}
    ...

This filters out sentences from the user ``sophia``. Any expression which
results in a ``bool`` value can be written in the ``WHERE`` clause.

Grouping and Aggregates
^^^^^^^^^^^^^^^^^^^^^^^

The ``GROUP BY`` clause is also available in BQL::

    wordcount> SELECT ISTREAM name, count(*) FROM sentences [RANGE 60 SECONDS]
        GROUP BY name;
    {"count":1,"name":"isabella"}
    {"count":1,"name":"emma"}
    {"count":2,"name":"isabella"}
    {"count":1,"name":"jacob"}
    {"count":3,"name":"isabella"}
    ...
    {"count":23,"name":"jacob"}
    {"count":32,"name":"isabella"}
    {"count":33,"name":"isabella"}
    {"count":24,"name":"jacob"}
    {"count":14,"name":"sophia"}
    ...

This statement creates groups of users in a 60 second-long window. It returns
pairs of a user and the number of sentences that have been written by that user
in the past 60 seconds. In addition to ``count``, BQL also provides built-in
aggregate functions such as ``min``, ``max``, and so on.

Also note that the statement above uses ``ISTREAM`` rather than ``RSTREAM``. The
statement only reports a new count for an updated user while ``RSTREAM`` reports
counts for all users every time it receives a tuple. Seeing the example of
outputs from the statements with ``RSTREAM`` and ``ISTREAM`` makes it easier to
understand their behaviors. When the statement receives ``isabella``, ``emma``,
``isabella``, ``jacob``, and ``isabella`` in this order, ``RSTREAM`` reports
results as shown below (with some comments)::

    wordcount> SELECT RSTREAM name, count(*) FROM sentences [RANGE 60 SECONDS]
        GROUP BY name;
    -- receive "isabella"
    {"count":1,"name":"isabella"}
    -- receive "emma"
    {"count":1,"name":"isabella"}
    {"count":1,"name":"emma"}
    -- receive "isabella"
    {"count":2,"name":"isabella"}
    {"count":1,"name":"emma"}
    -- receive "jacob"
    {"count":2,"name":"isabella"}
    {"count":1,"name":"emma"}
    {"count":1,"name":"jacob"}
    -- receive "isabella"
    {"count":3,"name":"isabella"}
    {"count":1,"name":"emma"}
    {"count":1,"name":"jacob"}

On the other hand, ``ISTREAM`` only emits tuples updated in the current
resulting relation::

    wordcount> SELECT ISTREAM name, count(*) FROM sentences [RANGE 60 SECONDS]
        GROUP BY name;
    -- receive "isabella"
    {"count":1,"name":"isabella"}
    -- receive "emma", the count of "isabella" isn't updated
    {"count":1,"name":"emma"}
    -- receive "isabella"
    {"count":2,"name":"isabella"}
    -- receive "jacob"
    {"count":1,"name":"jacob"}
    -- receive "isabella"
    {"count":3,"name":"isabella"}

This is one typical situation where ``ISTREAM`` works well.

Tokenizing Sentences
--------------------

To perform word counting, sentences that are contained in ``sources`` need to be
split up into words. Imagine there was a user-defined function (UDF)
``tokenize(sentence)`` returning an array of strings::

    SELECT RSTREAM name, tokenize(text) AS words FROM sentences ...

A resulting tuple of this statement would look like::

    {
        "name": "emma",
        "words": ["exercitation", "ut", "sed", "aute", "ullamco", "aliquip"]
    }

However, to count words with the ``GROUP BY`` clause and the ``count`` function,
the tuple above further needs to be split into multiple tuples so that each tuple has
one word instead of an array::

    {"name": "emma", "word": "exercitation"}
    {"name": "emma", "word": "ut"}
    {"name": "emma", "word": "sed"}
    {"name": "emma", "word": "aute"}
    {"name": "emma", "word": "ullamco"}
    {"name": "emma", "word": "aliquip"}

With such a stream, the statement below could easily compute the count of each word::

    SELECT ISTREAM word, count(*) FROM some_stream [RANGE 60 SECONDS]
        GROUP BY word;

To create a stream like this from tuples emitted from ``sentences``, BQL
has the concept of a **user-defined stream-generating function (UDSF)**. A UDSF is able
to emit multiple tuples from one input tuple, something that cannot be done with the
``SELECT`` statement itself. The ``wordcount`` package from this tutorial provides
a UDSF ``wc_tokenizer(stream, field)``, where ``name`` is the name of the input
stream and ``field`` is the name of the field containing a sentence to be
tokenized. Both arguments need to be string values.

::

    wordcount> SELECT RSTREAM * FROM wc_tokenizer("sentences", "text") [RANGE 1 TUPLES];
    {"name":"ethan","text":"duis"}
    {"name":"ethan","text":"lorem"}
    {"name":"ethan","text":"adipiscing"}
    {"name":"ethan","text":"velit"}
    {"name":"ethan","text":"dolor"}
    ...

In this example, ``wc_tokenizer`` receives tuples from the ``sentences`` stream
and tokenizes sentences contained in the ``text`` field of input tuples. Then,
it emits each tokenized word as a separate tuple.

.. note::

    As shown above, a UDSF is one of the most powerful tools to extend BQL's
    capability. It can virtually do anything that can be done for stream data.
    To learn how to develop it, see :ref:`server_programming_go_udsfs`.

Creating a Stream
-----------------

Although it is now possible to count tokenized words, it is easier to have something like
a "view" in SQL to avoid writing ``wc_tokenizer("sentences", "text")`` every time
issuing a new query. BQL has a **stream** (a.k.a a **continuous view**), which
just works like a view in RDBMSs. A stream can be created using the
``CREATE STREAM`` statement::

    wordcount> CREATE STREAM words AS
        SELECT RSTREAM name, text AS word
        FROM wc_tokenizer("sentences", "text") [RANGE 1 TUPLES];
    wordcount>

This statement creates a new stream called ``words``. The stream renames
``text`` field to ``word``. The stream can be referred by the ``FROM`` clause
of the ``SELECT`` statement as follows::

    wordcount> SELECT RSTREAM * FROM words [RANGE 1 TUPLES];
    {"name":"isabella","word":"pariatur"}
    {"name":"isabella","word":"adipiscing"}
    {"name":"isabella","word":"id"}
    {"name":"isabella","word":"et"}
    {"name":"isabella","word":"aute"}
    ...

A stream can be specified in the ``FROM`` clause of multiple ``SELECT``
statements and all those statements will receive the same tuples from
the stream.

Counting Words
--------------

After creating the ``words`` stream, words can be counted as follows::

    wordcount> SELECT ISTREAM word, count(*) FROM words [RANGE 60 SECONDS]
        GROUP BY word;
    {"count":1,"word":"aute"}
    {"count":1,"word":"eu"}
    {"count":1,"word":"quis"}
    {"count":1,"word":"adipiscing"}
    {"count":1,"word":"ut"}
    ...
    {"count":47,"word":"mollit"}
    {"count":35,"word":"tempor"}
    {"count":100,"word":"in"}
    {"count":38,"word":"sint"}
    {"count":79,"word":"dolor"}
    ...

This statement counts the number of occurrences of each word that appeared in the past 60
seconds. By creating another stream based on the ``SELECT`` statement above,
further statistical information can be obtained::

    wordcount> CREATE STREAM word_counts AS
        SELECT ISTREAM word, count(*) FROM words [RANGE 60 SECONDS]
        GROUP BY word;
    wordcount> SELECT RSTREAM max(count), min(count)
        FROM word_counts [RANGE 60 SECONDS];
    {"max":52,"min":52}
    {"max":120,"min":52}
    {"max":120,"min":50}
    {"max":165,"min":50}
    {"max":165,"min":45}
    ...
    {"max":204,"min":31}
    {"max":204,"min":30}
    {"max":204,"min":29}
    {"max":204,"min":28}
    {"max":204,"min":27}
    ...

The ``CREATE STREAM`` statement above creates a new stream ``word_counts``. The
next ``SELECT`` statement computes the maximum and minimum counts over words
observed in past 60 seconds.

Using a BQL File
----------------

All statements above will be cleared once the SensorBee server is restarted. By
using a BQL file, a topology can be set up on each startup of the server. A BQL
file can contain multiple BQL statements. For the statements used in this tutorial,
the file would look as follows::

    CREATE SOURCE sentences TYPE wc_sentences;

    CREATE STREAM words AS
        SELECT RSTREAM name, text AS word
            FROM wc_tokenizer("sentences", "text") [RANGE 1 TUPLES];

    CREATE STREAM word_counts AS
        SELECT ISTREAM word, count(*)
            FROM words [RANGE 60 SECONDS]
            GROUP BY word;

.. note::

    A BQL file cannot have the ``SELECT`` statement because it runs
    continuously until it is manually stopped.

To run the BQL file on the server, a configuration file for ``sensorbee run``
needs to be provided in YAML format. The name of the configuration file is often
``sensorbee.yaml``. For this tutorial, the file has the following content::

    topologies:
      wordcount:
        bql_file: wordcount.bql

``topologies`` is one of the top-level parameters related to topologies in
the server. It has names of topologies to be created on startup. In the file
above, there's only one topology ``wordcount``. Each topology has a ``bql_file``
parameter to specify which BQL file to execute. The ``wordcount.bql`` file was
copied to the current directory before and the configuration file above specifies it.

With this configuration file, the SensorBee server can be started as follows::

    /path/to/wordcount$ ./sensorbee run -c sensorbee.yaml
    ./sensorbee run -c sensorbee.yaml
    INFO[0000] Setting up the server context                 config={"logging":
    {"log_dropped_tuples":false,"min_log_level":"info","summarize_dropped_tuples":
    false,"target":"stderr"},"network":{"listen_on":":15601"},"storage":{"uds":
    {"params":{},"type":"in_memory"}},"topologies":{"wordcount":{"bql_file":
    "wordcount.bql"}}}
    INFO[0000] Setting up the topology                       topology=wordcount
    INFO[0000] Starting the server on :15601

As written in log messages, the topology ``wordcount`` is created before
the server actually starts.

Summary
-------

This tutorial provided a brief overview of SensorBee through word counting.
First of all, it showed how to build a custom ``sensorbee`` command to work with
the tutorial. Second, running the server and setting up a topology with
BQL was explained. Then, querying streams and how to create a new stream
using ``SELECT`` was introduced. Finally, word counting was performed over a
newly created stream and BQL statements that create a source and streams were
persisted in a BQL file so that the server can re-execute those statements on
startup.

In subsequent sections, there are more tutorials and samples to learn how
to integrate SensorBee with other tools and libraries.

.. todo:: add Advanced Examples

   Advanced Examples
   =================

   Querying With WebSocket From JavaScript
   ---------------------------------------
