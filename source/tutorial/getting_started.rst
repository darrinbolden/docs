***************
Getting Started
***************

.. todo::

    Add typical errors on each step and how to handle them

To work with SensorBee, this chapter introduces word counting as the first
tutorial. It covers the following topics::

* how to install and set up SensorBee
* how to build a custom ``sensorbee`` command
* how to use the ``sensorbee`` command
* how to query the SensorBee server with ``sensorbee shell`` and BQL

Prerequisites
=============

SensorBee requires Go 1.4 or later to be installed. Also, its development
environment needs to be set up correctly.

This tutorial assumes that readers know about basic Linux commands and basics
of SQL.

SensorBee itself doesn't have to be installed at this point.

.. todo::

    Explain more about Go in detail because not all users of SensorBee are
    familiar with Go although developers needs to know it.


Word Count Example
==================

As the first tutorial, this section shows a word count example. All programs
and configuration files required for this tutorial are provided in the
``wordcount`` package of the github repository
`<https://github.com/sensorbee/tutorial>`_.

Installing Word Count Example Package
-------------------------------------

The first thing that needs to be done is to ``go get`` the word count example
package in the repository::

    $ go get github.com/sensorbee/tutorial/wordcount

This command checks out the repository at
``$GOPATH/src/github.com/sensorbee/tutorial/wordcount``. The repository also
includes configuration files for building and running SensorBee. After
``go get`` successfully downloaded the package, copy those configuration files
in the ``config`` directory under the ``wordcount`` package to another temporary
directory (**replace /path/to/ with an appropriate path**)::

    $ mkdir -p /path/to/wordcount
    $ cp $GOPATH/src/github.com/sensorbee/tutorial/wordcount/config/* \
        /path/to/wordcount/
    $ ls /path/to/wordcount
    build.yaml
    sensorbee.yaml
    wordcount.bql

Everything necessary to try this tutorial is ready now except SensorBee. The
next step is to build a custom ``sensorbee`` command that includes plugins
for this tutorial.

Building a ``sensorbee`` Command
--------------------------------

To build a ``sensorbee`` command, ``build_sensorbee`` command needs to be
installed::

    $ go get gopkg.in/sensorbee/sensorbee.v0/cmd/build_sensorbee

This command is used to build a custom ``sensorbee`` command with plugins
provided by developers.

Then, move to the directory that has configuration files previously copied from
the tutorial package and execute ``build_sensorbee``::

    $ cd /path/to/wordcount
    /path/to/wordcount$ build_sensorbee
    /path/to/wordcount$ ls
    build.yaml
    sensorbee
    sensorbee.yaml
    sensorbee_main.go
    wordcount.bql

There're two new files in the directory: ``sensorbee`` and
``sensorbee_main.go``. Both of them are automatically generated by the
``build_sensorbee`` command. ``sensorbee`` is the command to run the SnesorBee
server or shell. The command is built from ``sensorbee_main.go`` by ``go build``.

``build_sensorbee`` builds a ``sensorbee`` command based on ``build.yaml``::

    /path/to/wordcount$ cat build.yaml
    plugins:
      - github.com/sensorbee/tutorial/wordcount/plugin

Inserting a new go path to the ``plugin`` section adds a new plugin to the
``sensorbee`` command, but this tutorial only uses the wordcount plugin above.
Other tutorials will cover this topic.

Run the Server
--------------

After building the ``sensorbee`` command having plugins for this tutorial,
run it as a server::

    /path/to/wordcount$ ./sensorbee run
    INFO[0000] Setting up the server context
    INFO[0000] Starting the server on :15601

``sensorbee run`` runs the SensorBee server. It writes some log messages to
stdout but they can be ignored at the moment. It provides HTTP JSON API and
listens on ``:15601`` by default. However, the API isn't directly used in this
tutorial. Instead of controlling the server by the API, this tutorial shows
how to use the ``sensorbee`` command and the **BQL** language, which is similar
to SQL but has some extensions for streaming data, to manipulate the server via
``sensorbee shell``.

To test if the server has successfully started, run the following command in
another terminal::

    $ curl http://localhost:15601/api/v1/runtime_status
    {"gomaxprocs":1,"goroot":"/home/pfn/go","goversion":"go1.4.2",
    "hostname":"sensorbee-tutorial","num_cgo_call":0,"num_cpu":4,
    "num_goroutine":13,"pid":33267,"user":"pfn",
    "working_directory":"/path/to/wordcount/"}

The server is correctly working if a response like this returned.

.. todo::

    Provide a /api/v1/status or something like /api/v1/ping to make the document
    simpler.

Setting Up a Topology
---------------------

Once the server has started, open another window or use screen/tmux to have
another terminal to manipulate the server. The server does nothing just after
it started up. There're a few steps required to be done to enjoy interacting
with stream data.

Firstly, to allow the server to process some stream data, it needs to have
a **topology**. A topology is a similar concept to a database in RDBMSs. It has
processing components such as data sources, continuous views, and so on.
Use ``sensorbee topology create`` commands to create a new topology
``wordcount`` for the tutorial::

    /path/to/wordcount$ ./sensorbee topology create wordcount
    /path/to/wordcount$ echo $?
    0

``$?`` will be ``0`` if the command was successful. Otherwise, it'll be
non-zero.

.. note::

    Almost everything in SensorBee is volatile at the moment and it's reset
    every time the server restarts. A topology is dropped when the server shuts
    down, too. Therefore, ``sensorbee topology create wordcount`` needs to be
    run on each startup of the server until we specify a config file to
    ``sensorbee run`` later.

Secondly, start ``sensorbee shell``::

    /path/to/wordcount$ ./sensorbee shell -t wordcount
    (wordcount)>>>

``-t wordcount`` means that the shell connects to ``wordcount`` topology. Now,
it's ready to try some BQL statements. To start with, try the ``EVAL``
statement, which evaluates arbitrary expressions supported by BQL::

    (wordcount)>>> EVAL 1 + 1;
    2
    (wordcount)>>> EVAL power(2.0, 2.5);
    5.65685424949238
    (wordcount)>>> EVAL 'Hello' || ', world!';
    Hello, world!

BQL also supports one line comments::

    (wordcount)>>> -- This is a comment
    (wordcount)>>>

Finally, create a source, which generates stream data or inputs data from other
stream data sources::

    (wordcount)>>> CREATE SOURCE sentences TYPE wc_sentences;
    (wordcount)>>>

This ``CREATE SOURCE`` statement creates a source named ``sentences``. Its type
is ``wc_sentencese`` and it's provided as a plugin in the ``wordcount`` package.
This source emits, on a regular basis, a random sentence having several words
with the name of a person who wrote a sentence. To receive data (i.e. tuples)
emitted from the source, use the ``SELECT`` statement::

    (wordcount)>>> SELECT RSTREAM * FROM sentences [RANGE 1 TUPLES];
    {"name":"isabella","text":"dolor consequat ut in ad in"}
    {"name":"sophia","text":"excepteur deserunt officia cillum lorem excepteur"}
    {"name":"sophia","text":"exercitation ut sed aute ullamco aliquip"}
    {"name":"jacob","text":"duis occaecat culpa dolor veniam elit"}
    {"name":"isabella","text":"dolore laborum in consectetur amet ut nostrud ullamco"}
    ...

Type ``C-c`` to stop the statement. Details of the statement isn't described for
now, but this is basically same as the ``SELECT`` statement in SQL except two
things: ``RSTREAM`` and ``RANGE``. Those concepts will briefly be explained in
the next section.

.. note::

    ``sensorbee shell`` prints a tuple in JSON format. Therefore, double quotes
    are used for strings instead of single quotes that is used in BQL. As a
    result, output of the ``SELECT`` statement in ``sensorbee shell`` cannot
    directly be copied to BQL statements.

Querying: Basics
----------------

This subsection introduces basics of querying, i.e., the ``SELECT`` statement.
Since it is very similar to SQL's ``SELECT``, two concepts that don't exist in
SQL is first described. Then, some features that are also provided in SQL will
be covered.

Stream-Related Operators
^^^^^^^^^^^^^^^^^^^^^^^^

BQL's ``SELECT`` statement has two concept for stream data processing:
**stream-to-relation operators** and **relation-to-stream operators**.

.. note::

    Skip the description of stream-to-relations and relation-to-stream operators
    if these aren't clear enough at the moment.

A stream-to-relation operator is a operator that literally converts a stream of
tuples to relations (i.e. a records in a table of the database). What it
actually does is to define a window having a finite set of tuples on a stream.
The operator is written as ``[RANGE n TUPLES]`` or ``[RANGE n SECONDS]``.
``[RANGE n TUPLES]`` creates a window having :math:`n` tuples.
``[RANGE n SECONDS]``, on the other hand, creates a window holding tuples
observed in past :math:`n` seconds (more precisely, the duration between the
oldest and newest tuple is at most :math:`n` seconds).

::

    SELECT RSTREAM * FROM sentences [RANGE 1 TUPLES];

In the previous example, it uses a stream-to-relation operator
``[RANGE 1 TUPLES]``. That means the window only have one tuple in it. The
resulting relation is computed based on windows defined in the statement.

Another concept that doesn't exist in SQL is relation-to-stream operator. It
converts a relation, which is a result of the ``SELECT`` statement, to a stream
of tuples. There're three types of operators:

* ``RSTREAM``
* ``ISTREAM``
* ``DSTREAM``

``RSTREAM`` emits all tuples in the relation resulting every time a new tuple
arrives and the result is updated. ``ISTREAM`` only emits tuples that are in the
current window and weren't in the previous window, that is, it emits tuples
having newly been inserted into the current relation. ``DSTREAM`` only emits
tuples in the previous relation, that is, it emits tuples deleted in the current
relation.

In the previous example, ``RSTREAM`` is used as a relation-to-stream operator.
Since the resulting relation is same as the input relation (i.e. window), it
only has one tuple in it.

.. note::

    The difference between using ``RSTREAM`` and ``ISTREAM`` should be described
    a little here. Considering the following resulting relation::

        t1: {'a': 1}
        t2: {'a': 2}
        t3: {'a': 2}
        t4: {'a': 3}

    ``RSTREAM`` emits four tuples as a result. On the other hand, ``ISTREAM``
    emits only three tuples::

        t1: {'a': 1}
        t2: {'a': 2}
        t4: {'a': 3}

    The reason why it happens is that the resulting relation wasn't updated at
    :math:`t_3` since both relations at :math:`t_2` and :math:`t_3` have
    the same tuple ``{'a': 2}`` as a result.

    In other words, when using ``ISTREAM`` with ``[RANGE 1 TUPLES]``, a
    resulting tuple is emitted only when it's different from the previous
    resulting tuple. On the other hand, ``RSTREAM`` emits the resulting tuple
    every time regardless of its value.

    Therefore, when the stream-to-relation operator is ``[RANGE 1 TUPLES]``,
    basically prefer ``RSTREAM`` to ``ISTREAM`` unless there's a strong reason
    to use ``ISTREAM``. It leads to less confusing results.

To learn more about these operators, see :ref:`bql_queries` after finishing this
tutorial.

Selection
^^^^^^^^^

The ``SELECT`` statement can partially pick up some fields of input tuples::

    (wordcount)>>> SELECT RSTREAM name FROM sentences [RANGE 1 TUPLES];
    {"name":"isabella"}
    {"name":"isabella"}
    {"name":"jacob"}
    {"name":"isabella"}
    {"name":"jacob"}
    ...

In this example, only the ``name`` field is picked up from input tuples that
have 'name' and 'text' fields.

BQL is schema-less at the moment and the specification of output tuples needs
to be documented by authors of sources. The ``SELECT`` statement is only able
to report an error at runtime instead of reporting it before it actually starts
running. This is also a drawback of being schema-less.

Filtering
^^^^^^^^^

The ``SELECT`` statement supports filtering with the ``WHERE`` clause as SQL
does::

    (wordcount)>>> SELECT RSTREAM * FROM sentences [RANGE 1 TUPLES] WHERE name = 'sophia';
    {"name":"sophia","text":"anim eu occaecat do est enim do ea mollit"}
    {"name":"sophia","text":"cupidatat et mollit consectetur minim et ut deserunt"}
    {"name":"sophia","text":"elit est laborum proident deserunt eu sed consectetur"}
    {"name":"sophia","text":"mollit ullamco ut sunt sit in"}
    {"name":"sophia","text":"enim proident cillum tempor esse occaecat exercitation"}
    ...

This filters out sentences from the user ``sophia``. Any expression which
results in a ``bool`` value can be written in the ``WHERE`` clause.

Grouping and Aggregates
^^^^^^^^^^^^^^^^^^^^^^^

The ``GROUP BY`` clause is also available in BQL::

    (wordcount)>>> SELECT ISTREAM name, count(*) FROM sentences [RANGE 60 SECONDS]
    ... GROUP BY name; -- '...' at the beginning of this line was inserted by the shell
    {"count":1,"name":"isabella"}
    {"count":1,"name":"emma"}
    {"count":2,"name":"isabella"}
    {"count":1,"name":"jacob"}
    {"count":3,"name":"isabella"}
    ...
    {"count":23,"name":"jacob"}
    {"count":32,"name":"isabella"}
    {"count":33,"name":"isabella"}
    {"count":24,"name":"jacob"}
    {"count":14,"name":"sophia"}
    ...

This statement creates groups of users in a 60 second-long window. It returns
pairs of a user and the number of sentences that have been written by the user
in past 60 seconds. In addition to ``count``, BQL also provides built-in
aggregate functions such as ``min``, ``max``, and so on.

Also note that the statement above uses ``ISTREAM`` rather than ``RSTREAM``. The
statement only reports a new count for an updated user while ``RSTREAM`` reports
counts for all users every time it receives a tuple. Seeing the example of
outputs from the statements with ``RSTREAM`` and ``ISTREAM`` makes it easier to
understand their behaviors. When the statement receives ``isabella``, ``emma``,
``isabella``, ``jacob``, and ``isabella`` in this order, ``RSTREAM`` reports
results as shown below (with some comments)::

    (wordcount)>>> SELECT RSTREAM name, count(*) FROM sentences [RANGE 60 SECONDS]
    ... GROUP BY name;
    -- receive 'isabella'
    {"count":1,"name":"isabella"}
    -- receive 'emma'
    {"count":1,"name":"isabella"}
    {"count":1,"name":"emma"}
    -- receive 'isabella'
    {"count":2,"name":"isabella"}
    {"count":1,"name":"emma"}
    -- receive 'jacob'
    {"count":2,"name":"isabella"}
    {"count":1,"name":"emma"}
    {"count":1,"name":"jacob"}
    -- receive 'isabella'
    {"count":3,"name":"isabella"}
    {"count":1,"name":"emma"}
    {"count":1,"name":"jacob"}

On the other hand, ``ISTREAM`` only emits tuples updated in the current
resulting relation::

    (wordcount)>>> SELECT ISTREAM name, count(*) FROM sentences [RANGE 60 SECONDS]
    ... GROUP BY name;
    -- receive 'isabella'
    {"count":1,"name":"isabella"}
    -- receive 'emma', the count of 'isabella' isn't updated
    {"count":1,"name":"emma"}
    -- receive 'isabella'
    {"count":2,"name":"isabella"}
    -- receive 'jacob'
    {"count":1,"name":"jacob"}
    -- receive 'isabella'
    {"count":3,"name":"isabella"}

This is one typical situation that ``ISTREAM`` works well.

Tokenizing Sentences
--------------------

To perform word counting, sentences that contained in ``sources`` needs to be
split up into words. There could be a user-defined function (UDF)
``tokenize(sentence)`` that returns an array of strings::

    SELECT RSTREAM name, tokenize(text) AS words FROM sentences ...

A resulting tuple of this statement may look like::

    {
        'name': 'emma',
        'words': ['exercitation', 'ut', 'sed', 'aute', 'ullamco', 'aliquip']
    }

However, to count words with the ``GROUP BY`` clause and the ``count`` function,
the tuple above further needs to be split up into tuples so that each tuple has
one word instead of an array of words::

    {'name': 'emma', 'word': 'exercitation'}
    {'name': 'emma', 'word': 'ut'}
    {'name': 'emma', 'word': 'sed'}
    {'name': 'emma', 'word': 'aute'}
    {'name': 'emma', 'word': 'ullamco'}
    {'name': 'emma', 'word': 'aliquip'}

With these results, the statement below can compute a count of each word::

    SELECT ISTREAM word, count(*) FROM some_stream [RANGE 60 SECONDS]
        GROUP BY word;

To create a stream like this from tuples emitted from ``sentences``, BQL
provides a **user-defined stream-generating function (UDSF)**. A UDSF is able
to emit multiple tuples from one input tuple, that cannot be done with the
``SELECT`` statement itself. ``wordcount`` package in this tutorial provides
a UDSF ``wc_tokenizer(stream, field)``: where ``name`` is the name of the input
stream and ``field`` is the name of the field containing a sentence to be
tokenized. Both arguments need to be string values.

::

    (wordcount)>>> SELECT RSTREAM * FROM wc_tokenizer('sentences', 'text') [RANGE 1 TUPLES];
    {"name":"ethan","text":"duis"}
    {"name":"ethan","text":"lorem"}
    {"name":"ethan","text":"adipiscing"}
    {"name":"ethan","text":"velit"}
    {"name":"ethan","text":"dolor"}
    ...

In this example, ``wc_tokenizer`` receives tuples from the ``sentences`` stream
and tokenizes sentences contained in the ``text`` field of input tuples. Then,
it emits each tokenized word as a separated tuple.

.. note::

    As shown above, a UDSF is one of the most powerful tools to extend BQL's
    capability. It can virtually do anything that can be done for stream data.
    To learn how to develop it, see :ref:`server_programming_go_udsfs`.

Creating a Stream
-----------------

Although it's ready to count tokenized words, it's easier to have something like
a view to avoid writing ``wc_tokenizer('sentences', 'text')`` every time
issuing a new query. BQL has a **stream** (a.k.a a **continuous view**), which
just works like a view in RDBMSs. A stream can be created by the
``CREATE STREAM`` statement::

    (wordcount)>>> CREATE STREAM words AS
    ... SELECT RSTREAM name, text AS word
    ... FROM wc_tokenizer('sentences', 'text') [RANGE 1 TUPLES];
    (wordcount)>>>

This statement creates a new stream called ``words``. The stream renames
``text`` field to ``word``. The stream can be referred by the ``FROM`` clause
of the ``SELECT`` statement as follows::

    (wordcount)>>> SELECT RSTREAM * FROM words [RANGE 1 TUPLES];
    {"name":"isabella","word":"pariatur"}
    {"name":"isabella","word":"adipiscing"}
    {"name":"isabella","word":"id"}
    {"name":"isabella","word":"et"}
    {"name":"isabella","word":"aute"}
    ...

A stream can be specified in the ``FROM`` clause of multiple ``SELECT``
statements so that it can fork as many as required.

Counting Words
--------------

After creating the ``words`` stream, words can be counted as follows::

    (wordcount)>>> SELECT ISTREAM word, count(*) FROM words [RANGE 60 SECONDS]
    ... GROUP BY word;
    {"count":1,"word":"aute"}
    {"count":1,"word":"eu"}
    {"count":1,"word":"quis"}
    {"count":1,"word":"adipiscing"}
    {"count":1,"word":"ut"}
    ...
    {"count":47,"word":"mollit"}
    {"count":35,"word":"tempor"}
    {"count":100,"word":"in"}
    {"count":38,"word":"sint"}
    {"count":79,"word":"dolor"}
    ...

This statement counts the number of occurrences of each word appeared in past 60
seconds. By creating another stream based on the ``SELECT`` statement above,
Further statistical information can be obtained::

    (wordcount)>>> CREATE STREAM word_counts AS
    ... SELECT ISTREAM word, count(*) FROM words [RANGE 60 SECONDS]
    ... GROUP BY word;
    (wordcount)>>> (wordcount)>>> SELECT RSTREAM max(count), min(count)
    ... FROM word_counts [RANGE 60 SECONDS];
    {"max":52,"min":52}
    {"max":120,"min":52}
    {"max":120,"min":50}
    {"max":165,"min":50}
    {"max":165,"min":45}
    ...
    {"max":204,"min":31}
    {"max":204,"min":30}
    {"max":204,"min":29}
    {"max":204,"min":28}
    {"max":204,"min":27}
    ...

The ``CREATE STREAM`` statement above creates a new stream ``word_counts``. The
next ``SELECT`` statement computes the maximum and minimum counts over words
observed in past 60 seconds.

Using a BQL File
----------------

All statements above will be cleared once the SensorBee server is restarted. By
using a BQL file, a topology can be set up on each startup of the server. A BQL
file can contain multiple BQL statements. For statements used in this tutorial,
the file can contain following statements::

    CREATE SOURCE sentences TYPE wc_sentences;

    CREATE STREAM words AS
        SELECT RSTREAM name, text AS word
            FROM wc_tokenizer('sentences', 'text') [RANGE 1 TUPLES];

    CREATE STREAM word_counts AS
        SELECT ISTREAM word, count(*)
            FROM words [RANGE 60 SECONDS]
            GROUP BY word;

.. note::

    A BQL file cannot have the ``SELECT`` statement because it doesn't stop
    until it's manually stopped.

To apply the BQL file to the server, a configuration file for ``sensorbee run``
needs to be provided in YAML format. The name of the configuration file is often
``sensorbee.yaml``. For this tutorial, the file has the following content::

    topologies:
      wordcount:
        bql_file: wordcount.bql

``topologies`` is one of the top-level parameters related to topologies in
the server. It has names of topologies to be created on startup. In the file
above, there's only one topology ``wordcount``. Each topology has ``bql_file``
parameter to specify which BQL file to be executed. There's ``wordcount.bql``
in the ``config`` directly and the configuration file above specifies it.

With this configuration file, the SensorBee server can be started as follows::

    /path/to/wordcount$ ./sensorbee run -c sensorbee.yaml
    INFO[0000] Setting up the server context
    INFO[0000] Setting up the topology                       topology=wordcount
    INFO[0000] Starting the server on :15601

As it's written in log messages, the topology ``wordcount`` is created before
the server actually starts.

Summary
-------

This tutorial provided a brief overview of SensorBee through word counting.
First of all, it showed how to build a custom ``sensorbee`` command to work with
the tutorial. Second of all, running the server and setting up a topology with
BQL was explained. Then, querying toward streams and how to create a new stream
from ``SELECT`` was introduced. Finally, word counting was performed over a
newly created stream and BQL statements that create a source and streams were
persistent in a BQL file so that the server can re-execute those statements on
its startup.

This chapter introduced the first tutorial and there're other tutorials and
samples to learn how to integrate SensorBee with other tools and libraries.

Advanced Examples
=================

Querying With WebSocket From JavaScript
---------------------------------------
